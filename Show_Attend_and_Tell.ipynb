{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28D-rC5iv6H3"
      },
      "source": [
        "# Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6SuLCG6xLnn"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!unzip 'Flickr8k_Dataset.zip'\n",
        "!rm -r 'Flickr8k_Dataset.zip'\n",
        "\n",
        "!wget http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
        "!unzip 'caption_datasets.zip'\n",
        "!rm -r 'caption_datasets.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEedqgSzkALh"
      },
      "outputs": [],
      "source": [
        "!rm dataset_flickr30k.json\n",
        "!rm dataset_coco.json\n",
        "!rm -r sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVjs80SLPj46"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oC9KVGnMu6X"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "import skimage.transform\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38A3q_mS_LAL"
      },
      "source": [
        "## Data Preprocessing and Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9nmrQGG6Yc8"
      },
      "outputs": [],
      "source": [
        "# sets device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMHLqqS_RXsE"
      },
      "outputs": [],
      "source": [
        "id2word = {}\n",
        "def build_word_dict(captions, min_word_count):\n",
        "    \"\"\"\n",
        "    Builds a word dictionary from caption tokens, filtering by minimum count.\n",
        "    \"\"\"\n",
        "    word_count = Counter(token for caption in captions for token in caption)\n",
        "    vocab = [word for word, count, in word_count.items() if count>= min_word_count]\n",
        "\n",
        "    word_dict = {word: idx+4 for idx, word in enumerate(vocab)}\n",
        "    word_dict['<start>'] = 0\n",
        "    word_dict['<eos>'] = 1\n",
        "    word_dict['<unk>'] = 2\n",
        "    word_dict['<pad>'] = 3\n",
        "\n",
        "    return word_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT8wKv1xz7qF"
      },
      "outputs": [],
      "source": [
        "def tokenize_captions(captions, word_dict, max_length):\n",
        "    \"\"\"\n",
        "    Converts list of tokenized captions to indexed and padded format.\n",
        "    \"\"\"\n",
        "    indexed, caps_len = [], []\n",
        "    for tokens in captions:\n",
        "        idxs = [word_dict.get(token, word_dict['<unk>']) for token in tokens]\n",
        "        padded = [word_dict['<start>']] + idxs + [word_dict['<eos>']]\n",
        "        padded += [word_dict['<pad>']] * (max_length - len(tokens))\n",
        "        indexed.append(padded)\n",
        "        caps_len.append(len(tokens) + 2)\n",
        "    return indexed, caps_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4wHIXy96d3i"
      },
      "outputs": [],
      "source": [
        "def generate_json_data(split_path, data_path, min_word_count, max_caption_len=100):\n",
        "    \"\"\"\n",
        "    Splits data into train, val, test dataset and saves as JSON\n",
        "    \"\"\"\n",
        "    with open(split_path, 'r') as f:\n",
        "        split_data = json.load(f)\n",
        "\n",
        "    train_img_paths, val_img_paths, test_img_paths = [], [], []\n",
        "    train_captions, val_captions, test_captions = [], [], []\n",
        "\n",
        "    for img in split_data['images']:\n",
        "        for sentence in img['sentences']:\n",
        "            img_path = os.path.join(data_path, img['filename'])\n",
        "            tokens = sentence['tokens']\n",
        "\n",
        "            if img['split'] == 'train':\n",
        "                train_img_paths.append(img_path)\n",
        "                train_captions.append(tokens)\n",
        "            elif img['split'] == 'val':\n",
        "                val_img_paths.append(img_path)\n",
        "                val_captions.append(tokens)\n",
        "            elif img['split'] == 'test':\n",
        "                test_img_paths.append(img_path)\n",
        "                test_captions.append(tokens)\n",
        "\n",
        "    word_dict = build_word_dict(train_captions + val_captions + test_captions, min_word_count)\n",
        "\n",
        "    train_caption_ids, train_caption_lens = tokenize_captions(train_captions, word_dict, max_caption_len)\n",
        "    val_caption_ids, val_caption_lens = tokenize_captions(val_captions, word_dict, max_caption_len)\n",
        "    test_caption_ids, test_caption_lens = tokenize_captions(test_captions, word_dict, max_caption_len)\n",
        "\n",
        "    # Output paths\n",
        "    with open(os.path.join(data_path, 'word_dict.json'), 'w') as f:\n",
        "        json.dump(word_dict, f)\n",
        "\n",
        "    with open(os.path.join(data_path, 'train_img_paths.json'), 'w') as f:\n",
        "        json.dump(train_img_paths, f)\n",
        "    with open(os.path.join(data_path, 'val_img_paths.json'), 'w') as f:\n",
        "        json.dump(val_img_paths, f)\n",
        "    with open(os.path.join(data_path, 'test_img_paths.json'), 'w') as f:\n",
        "        json.dump(test_img_paths, f)\n",
        "\n",
        "    with open(os.path.join(data_path, 'train_captions.json'), 'w') as f:\n",
        "        json.dump(train_caption_ids, f)\n",
        "    with open(os.path.join(data_path, 'val_captions.json'), 'w') as f:\n",
        "        json.dump(val_caption_ids, f)\n",
        "    with open(os.path.join(data_path, 'test_captions.json'), 'w') as f:\n",
        "        json.dump(test_caption_ids, f)\n",
        "\n",
        "    with open(os.path.join(data_path, 'train_caption_lens.json'), 'w') as f:\n",
        "        json.dump(train_caption_lens, f)\n",
        "    with open(os.path.join(data_path, 'val_caption_lens.json'), 'w') as f:\n",
        "        json.dump(val_caption_lens, f)\n",
        "    with open(os.path.join(data_path, 'test_caption_lens.json'), 'w') as f:\n",
        "        json.dump(test_caption_lens, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gRacREce1zc"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/Flicker8k_Dataset'\n",
        "min_word_freq = 5\n",
        "split_path = 'dataset_flickr8k.json'\n",
        "\n",
        "generate_json_data(split_path, data_path, min_word_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyfslklp5no6"
      },
      "outputs": [],
      "source": [
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, data_path, split, transform, num_captions=5):\n",
        "        self.img_paths = json.load(open(os.path.join(data_path, f'{split}_img_paths.json'), 'r'))\n",
        "        self.captions = json.load(open(os.path.join(data_path, f'{split}_captions.json'), 'r'))\n",
        "        self.caption_lens = json.load(open(os.path.join(data_path, f'{split}_caption_lens.json'), 'r'))\n",
        "        self.num_captions = num_captions\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.split == 'train':\n",
        "            return torch.FloatTensor(img), torch.tensor(self.captions[idx]), torch.tensor(self.caption_lens[idx]).reshape(-1)\n",
        "\n",
        "        all_captions = [self.captions[i] for i, path in enumerate(self.img_paths) if path == img_path]\n",
        "\n",
        "        return torch.FloatTensor(img), torch.tensor(self.captions[idx]), torch.tensor(self.caption_lens[idx]).reshape(-1), torch.tensor(all_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABxVv9Ah57f1"
      },
      "outputs": [],
      "source": [
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "batch_size = 32\n",
        "data_path = '/content/Flicker8k_Dataset'\n",
        "\n",
        "train_dataset = CaptionDataset(data_path, 'train', img_transform)\n",
        "val_dataset = CaptionDataset(data_path, 'val', img_transform)\n",
        "test_dataset = CaptionDataset(data_path, 'test', img_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFPMIbc1pe7K"
      },
      "source": [
        "### Visualize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx6mHbU1tIrt"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(data_path, 'word_dict.json'), 'r') as f:\n",
        "    word2id = json.load(f)\n",
        "\n",
        "id2word = {v: k for k, v in word2id.items()}\n",
        "\n",
        "def decode_caption(caption, word_dict=word2id):\n",
        "    idx2word = {idx: word for word, idx in word2id.items()}\n",
        "    decoded_caption = []\n",
        "\n",
        "    for id in caption.numpy():\n",
        "        if idx2word[id] not in [\"<start>\", \"<eos>\", \"<unk>\", \"<pad>\"]:\n",
        "            decoded_caption.append(idx2word[id])\n",
        "\n",
        "    return \" \".join(decoded_caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaQTfm_VhO59"
      },
      "outputs": [],
      "source": [
        "img, caption, _ = train_dataset[100]\n",
        "\n",
        "print(decode_caption(caption))\n",
        "print()\n",
        "plt.imshow(img.permute(1, 2, 0));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHMJaK1Jp2eQ"
      },
      "outputs": [],
      "source": [
        "img, _, _, all_captions = val_dataset[120]\n",
        "\n",
        "for caption in all_captions:\n",
        "    print(decode_caption(caption))\n",
        "print()\n",
        "plt.imshow(img.permute(1, 2, 0));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0l7A7eRxuVdT"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "workers = 2\n",
        "\n",
        "# dataloader for train and val set\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=workers)\n",
        "\n",
        "# quick check to see if data is properly loaded or not\n",
        "next_images, next_captions, next_caption_lens = next(iter(train_loader))\n",
        "print(next_images.shape)\n",
        "print(next_captions.shape)\n",
        "print(next_caption_lens.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUl6vklGwrfU"
      },
      "source": [
        "## Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0UIx7howzKL"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import vgg19\n",
        "\n",
        "class VGGEncoder(nn.Module):\n",
        "  def __init__(self, output_size=14):\n",
        "    super(VGGEncoder, self).__init__()\n",
        "    self.output_size = output_size\n",
        "\n",
        "    vgg = vgg19(pretrained=True)\n",
        "    self.model = nn.Sequential(*list(vgg.children())[:-1])\n",
        "\n",
        "    self.pool = nn.AdaptiveAvgPool2d((output_size, output_size))\n",
        "    self.fine_tune()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.model(x)\n",
        "    out = self.pool(out)\n",
        "    out = out.permute(0, 2, 3, 1)\n",
        "    return out\n",
        "\n",
        "  def fine_tune(self, fine_tune=True):\n",
        "    for param in self.model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    if fine_tune:\n",
        "      for child in list(self.model.children())[5:]:\n",
        "        for param in child.parameters():\n",
        "          param.requires_grad = fine_tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2ZCVDNc9KT-"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet152\n",
        "class ResNetEncoder(nn.Module):\n",
        "  def __init__(self, output_size=14):\n",
        "    super(ResNetEncoder, self).__init__()\n",
        "    self.output_size = output_size\n",
        "\n",
        "    resnet = resnet152(pretrained=True)\n",
        "    self.model = nn.Sequential(*list(resnet.children())[:-2])\n",
        "\n",
        "    self.pool = nn.AdaptiveAvgPool2d((output_size, output_size))\n",
        "    self.fine_tune()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.model(x)\n",
        "    out = self.pool(out)\n",
        "    out = out.permute(0, 2, 3, 1)\n",
        "    return out\n",
        "\n",
        "  def fine_tune(self, fine_tune=True):\n",
        "    for param in self.model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    if fine_tune:\n",
        "      for child in list(self.model.children())[5:]:\n",
        "        for param in child.parameters():\n",
        "          param.requires_grad = fine_tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2oWIFHh9LFw"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import densenet201\n",
        "class ImageEncoder(nn.Module):\n",
        "  def __init__(self, output_size=14):\n",
        "    super(ImageEncoder, self).__init__()\n",
        "    self.output_size = output_size\n",
        "\n",
        "    densenet = densenet201(pretrained=True)\n",
        "    self.model = nn.Sequential(*list(densenet.children())[:-1])\n",
        "\n",
        "    self.pool = nn.AdaptiveAvgPool2d((output_size, output_size))\n",
        "    self.fine_tune()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.model(x)\n",
        "    out = self.pool(out)\n",
        "    out = out.permute(0, 2, 3, 1)\n",
        "    return out\n",
        "\n",
        "  def fine_tune(self, fine_tune=True):\n",
        "    for param in self.model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    if fine_tune:\n",
        "      for child in list(self.model.children())[-2:]:\n",
        "        for param in child.parameters():\n",
        "          param.requires_grad = fine_tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-kLSbCwCwFW"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "class GoogLeNetEncoder(nn.Module):\n",
        "  def __init__(self, output_size=14):\n",
        "    super(GoogLeNetEncoder, self).__init__()\n",
        "    self.output_size = output_size\n",
        "\n",
        "    googlenet = models.googlenet(pretrained=True)\n",
        "    self.model = nn.Sequential(*list(googlenet.children())[:-2])\n",
        "    self.pool = nn.AdaptiveAvgPool2d((output_size, output_size))\n",
        "    self.fine_tune()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.model(x)\n",
        "    out = self.pool(out)\n",
        "    out = out.permute(0, 2, 3, 1)\n",
        "    return out\n",
        "\n",
        "  def fine_tune(self, fine_tune=True):\n",
        "    for param in self.model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    if fine_tune:\n",
        "      for child in list(self.model.children())[5:]:\n",
        "        for param in child.parameters():\n",
        "          param.requires_grad = fine_tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dumSMpCCwFW"
      },
      "outputs": [],
      "source": [
        "class AlexNetEncoder(nn.Module):\n",
        "  def __init__(self, output_size=14):\n",
        "    super(AlexNetEncoder, self).__init__()\n",
        "    self.output_size = output_size\n",
        "\n",
        "    alexnet = models.alexnet(pretrained=True)\n",
        "    self.model = nn.Sequential(*list(alexnet.children())[:-3])\n",
        "    self.pool = nn.AdaptiveAvgPool2d((output_size, output_size))\n",
        "    self.fine_tune()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.model(x)\n",
        "    out = self.pool(out)\n",
        "    out = out.permute(0, 2, 3, 1)\n",
        "    return out\n",
        "\n",
        "  def fine_tune(self, fine_tune=True):\n",
        "    for param in self.model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    if fine_tune:\n",
        "      for child in list(self.model.children())[5:]:\n",
        "        for param in child.parameters():\n",
        "          param.requires_grad = fine_tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTq2_ozJ4TJi"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # (batch_size, num_pixels, attention_dim)\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # (batch_size, attention_dim)\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # (batch_size, num_pixels)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softmax = nn.Softmax(dim=1)  # (batch_size, num_pixels)\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        annotation_vec = self.encoder_att(encoder_out)  # project each image region feature a_i into the attention space\n",
        "        hidden_state = self.decoder_att(decoder_hidden)  # projects decoder's hidden state to the attention space\n",
        "        eti = self.full_att(self.tanh(annotation_vec + hidden_state.unsqueeze(1))).squeeze(2)  # e_{ti}\n",
        "        alpha = self.softmax(eti)  # softmax layer calculates attention weights\n",
        "        context_vec = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # z_t the context vector\n",
        "\n",
        "        return context_vec, alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP0SoSBK5lp3"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM decoder with attention for image captioning.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_dim: int,\n",
        "        embed_dim: int,\n",
        "        decoder_dim: int,\n",
        "        vocab_size: int,\n",
        "        encoder_dim: int = 2048,\n",
        "        dropout: float = 0.5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # feature sizes\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # modules\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.decoder_cell = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim)\n",
        "\n",
        "        # initialize LSTM from image features\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
        "\n",
        "        # gating for context vector\n",
        "        self.beta_gate = nn.Linear(decoder_dim, encoder_dim)\n",
        "\n",
        "        self.output_proj = nn.Linear(decoder_dim, vocab_size)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.output_proj.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.output_proj.bias.data.zero_()\n",
        "\n",
        "    def init_hidden_states(self, encoder_feats: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Create initial hidden & cell states from the mean encoder features.\n",
        "        :param encoder_feats: (batch, num_pixels, encoder_dim)\n",
        "        \"\"\"\n",
        "        mean_feats = encoder_feats.mean(dim=1)\n",
        "        return self.init_h(mean_feats), self.init_c(mean_feats)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        encoder_out: torch.Tensor,\n",
        "        captions: torch.Tensor,\n",
        "        caption_lengths: torch.Tensor,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param encoder_out: (batch, enc_size, enc_size, encoder_dim)\n",
        "        :param captions:    (batch, max_caption_len)\n",
        "        :param caption_lengths: (batch, 1)\n",
        "        :returns:\n",
        "          - predictions:      (batch, max_steps, vocab_size)\n",
        "          - sorted_captions:  (batch, max_caption_len)\n",
        "          - decode_lengths:   List[int]\n",
        "          - attention_weights:(batch, max_steps, num_pixels)\n",
        "          - sort_idx:         (batch,)\n",
        "        \"\"\"\n",
        "        batch_size = encoder_out.size(0)\n",
        "        # (batch, num_pixels, encoder_dim)\n",
        "        encoder_feats = encoder_out.view(batch_size, -1, self.encoder_dim)\n",
        "        num_pixels = encoder_feats.size(1)\n",
        "\n",
        "        # descending caption length\n",
        "        lengths, sort_idx = caption_lengths.squeeze(1).sort(descending=True)\n",
        "        encoder_feats = encoder_feats[sort_idx]\n",
        "        sorted_caps = captions[sort_idx]\n",
        "        decode_lengths = (lengths - 1).tolist()\n",
        "        max_steps = max(decode_lengths)\n",
        "\n",
        "        embed = self.embedding(sorted_caps)  # (batch, max_len, embed_dim)\n",
        "\n",
        "        # init LSTM state\n",
        "        hidden_state, cell_state = self.init_hidden_states(encoder_feats)\n",
        "\n",
        "        device = encoder_feats.device\n",
        "        preds = encoder_feats.new_zeros(batch_size, max_steps, self.vocab_size)\n",
        "        alphas = encoder_feats.new_zeros(batch_size, max_steps, num_pixels)\n",
        "\n",
        "        for t in range(max_steps):\n",
        "            active_mask = torch.tensor([l > t for l in decode_lengths], device=device)\n",
        "            if not active_mask.any():\n",
        "                break\n",
        "            idx = active_mask.nonzero(as_tuple=False).squeeze(1)\n",
        "\n",
        "            # attend\n",
        "            ctx, alpha = self.attention(encoder_feats[idx], hidden_state[idx])\n",
        "\n",
        "            # gate the context\n",
        "            gate = torch.sigmoid(self.beta_gate(hidden_state[idx]))\n",
        "            ctx = gate * ctx\n",
        "\n",
        "            # LSTMCell\n",
        "            lstm_input = torch.cat([embed[idx, t, :], ctx], dim=1)\n",
        "            h_t, c_t = self.decoder_cell(lstm_input, (hidden_state[idx], cell_state[idx]))\n",
        "            hidden_state[idx], cell_state[idx] = h_t, c_t\n",
        "\n",
        "            # project to vocab\n",
        "            step_preds = self.output_proj(self.dropout(h_t))\n",
        "            preds[idx, t, :] = step_preds\n",
        "            alphas[idx, t, :] = alpha.to(alphas.dtype)\n",
        "\n",
        "        return preds, sorted_caps, decode_lengths, alphas, sort_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt_Y6N6n3Cgc"
      },
      "source": [
        "## Utility functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsGy2NmM3E34"
      },
      "outputs": [],
      "source": [
        "def accuracy(scores, targets, k):\n",
        "    \"\"\"\n",
        "    Computes top-k accuracy, from predicted and true labels.\n",
        "    :param scores: scores from the model\n",
        "    :param targets: true labels\n",
        "    :param k: k in top-k accuracy\n",
        "    :return: top-k accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = targets.size(0)\n",
        "    _, ind = scores.topk(k, 1, True, True)\n",
        "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
        "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
        "    return correct_total.item() * (100.0 / batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiTPP0K77Z8E"
      },
      "outputs": [],
      "source": [
        "def clip_gradient(optimizer, grad_clip):\n",
        "    \"\"\"\n",
        "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
        "    :param optimizer: optimizer with the gradients to be clipped\n",
        "    :param grad_clip: clip value\n",
        "    \"\"\"\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ojswk9H0eKQn"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def save_checkpoint(\n",
        "    epoch: int,\n",
        "    epochs_since_improvement: int,\n",
        "    encoder: torch.nn.Module,\n",
        "    decoder: torch.nn.Module,\n",
        "    encoder_optimizer: torch.optim.Optimizer | None,\n",
        "    decoder_optimizer: torch.optim.Optimizer,\n",
        "    bleu_scores: list[float],\n",
        "    meteor_score: float,\n",
        "    val_acc: float,\n",
        "    train_acc: float,\n",
        "    is_best: bool,\n",
        "    ckpt_dir: str = \"checkpoints\",\n",
        "    prefix: str = \"image_captioning\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Save model + optimizer state_dicts.\n",
        "\n",
        "    Args:\n",
        "        epoch: current epoch (1-indexed).\n",
        "        epochs_since_improvement: epochs since last avg bleu score improvement.\n",
        "        encoder: the encoder network.\n",
        "        decoder: the decoder network.\n",
        "        encoder_optimizer: optimizer for encoder (or None).\n",
        "        decoder_optimizer: optimizer for decoder.\n",
        "        bleu_score: current BLEU scores.\n",
        "        meteor_score: current METEOR score.\n",
        "        val_acc: current validation accuracy,\n",
        "        train_acc: current training accuracy,\n",
        "        is_best: True if this is the best model so far.\n",
        "        ckpt_dir: directory to save checkpoints into.\n",
        "        prefix: filename prefix (no extension).\n",
        "    \"\"\"\n",
        "    ckpt_path = Path(ckpt_dir)\n",
        "    ckpt_path.mkdir(exist_ok=True)\n",
        "\n",
        "    state = {\n",
        "        \"epoch\": epoch,\n",
        "        \"epochs_since_improvement\": epochs_since_improvement,\n",
        "        \"bleu-score\": bleu_scores,\n",
        "        \"meteor-score\": meteor_score,\n",
        "        \"validation-accuracy\": val_acc,\n",
        "        \"training-accuracy\": train_acc,\n",
        "        \"encoder_state_dict\": encoder.state_dict(),\n",
        "        \"decoder_state_dict\": decoder.state_dict(),\n",
        "        \"decoder_optimizer_state_dict\": decoder_optimizer.state_dict(),\n",
        "    }\n",
        "    # Only include encoder optimizer if it exists\n",
        "    if encoder_optimizer is not None:\n",
        "        state[\"encoder_optimizer_state_dict\"] = encoder_optimizer.state_dict()\n",
        "\n",
        "    # Filename with zero-padded epoch number\n",
        "    fname = f\"{prefix}_epoch{epoch:02d}.pth\"\n",
        "    full_path = ckpt_path / fname\n",
        "    torch.save(state, full_path)\n",
        "    print(f\"Checkpoint saved: {full_path}\")\n",
        "\n",
        "    # If this is the best model so far, overwrite the \"best\" symlink\n",
        "    if is_best:\n",
        "        best_path = ckpt_path / f\"{prefix}_best.pth\"\n",
        "        torch.save(state, best_path)\n",
        "        print(f\"*** New best model saved: {best_path} ***\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyeoQJy-z-_u"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FMSki0Y0GyV"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "def train_epoch(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer):\n",
        "    losses = []\n",
        "    top5accs = []\n",
        "\n",
        "    decoder.train()\n",
        "    encoder.train()\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for i, (imgs, caps, cap_lens) in enumerate(tqdm(train_loader, total=len(train_loader))):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        caps = caps.to(device, non_blocking=True)\n",
        "        cap_lens = cap_lens.to(device, non_blocking=True)\n",
        "\n",
        "        decoder_optimizer.zero_grad()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            imgs = encoder(imgs)\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, cap_lens)\n",
        "            targets = caps_sorted[:, 1:]\n",
        "\n",
        "            scores_packed = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
        "            targets_packed = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
        "\n",
        "            loss = criterion(scores_packed, targets_packed) + alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if grad_clip is not None:\n",
        "            scaler.unscale_(decoder_optimizer)\n",
        "            clip_gradient(decoder_optimizer, grad_clip)\n",
        "            if encoder_optimizer is not None:\n",
        "                scaler.unscale_(encoder_optimizer)\n",
        "                clip_gradient(encoder_optimizer, grad_clip)\n",
        "\n",
        "        scaler.step(decoder_optimizer)\n",
        "        if encoder_optimizer is not None:\n",
        "            scaler.step(encoder_optimizer)\n",
        "\n",
        "        scaler.update()\n",
        "\n",
        "        top5 = accuracy(scores_packed, targets_packed, 5)\n",
        "        losses.append(loss.item())\n",
        "        top5accs.append(top5)\n",
        "\n",
        "    return np.mean(losses), np.mean(top5accs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhw1Whb18RjW"
      },
      "outputs": [],
      "source": [
        "# Run this cell to download WordNet\n",
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def compute_meteor_scores(references_ids, hypotheses_ids):\n",
        "    meteor_scores = []\n",
        "    # printed = False\n",
        "    for refs, hyp in zip(references_ids, hypotheses_ids):\n",
        "        # Convert refs and hyp to strings\n",
        "        refs_str = [[id2word[tok] for tok in ref] for ref in refs]\n",
        "        hyp_str = [id2word[tok] for tok in hyp]\n",
        "\n",
        "        score = meteor_score(refs_str, hyp_str)\n",
        "        meteor_scores.append(score)\n",
        "\n",
        "    return sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvtcojIdAPbm"
      },
      "outputs": [],
      "source": [
        "def val_epoch(val_loader, encoder, decoder, criterion):\n",
        "    losses = []\n",
        "    top5accs = []\n",
        "\n",
        "    decoder.eval()\n",
        "    if encoder is not None:\n",
        "        encoder.eval()\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (imgs, caps, cap_lens, all_caps) in enumerate(tqdm(val_loader, total=len(val_loader))):\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            caps = caps.to(device, non_blocking=True)\n",
        "            cap_lens = cap_lens.to(device, non_blocking=True)\n",
        "\n",
        "            imgs = encoder(imgs)\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, cap_lens)\n",
        "            targets = caps_sorted[:, 1:]\n",
        "\n",
        "            scores_copy = scores.clone()\n",
        "\n",
        "            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
        "            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
        "\n",
        "            loss = criterion(scores, targets) + alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "            top5 = accuracy(scores, targets, 5)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            top5accs.append(top5)\n",
        "\n",
        "            sort_ind = sort_ind.cpu()\n",
        "            all_caps = all_caps[sort_ind]\n",
        "            for j in range(all_caps.shape[0]):\n",
        "                img_caps = all_caps[j].tolist()\n",
        "                img_captions = list(map(\n",
        "                    lambda caption: [word for word in caption if word not in {word2id['<start>'], word2id['<pad>']}],\n",
        "                    img_caps))\n",
        "                references.append(img_captions)\n",
        "\n",
        "            _, preds = torch.max(scores_copy, dim=2)\n",
        "            preds = preds.tolist()\n",
        "            temp_preds = [preds[j][:decode_lengths[j]] for j in range(len(preds))]\n",
        "            hypotheses.extend(temp_preds)\n",
        "\n",
        "        assert len(references) == len(hypotheses)\n",
        "        bleu_scores = [corpus_bleu(references, hypotheses, weights=(1.0, 0, 0, 0)),\n",
        "                       corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0)),\n",
        "                       corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0)),\n",
        "                       corpus_bleu(references, hypotheses)\n",
        "                       ]\n",
        "        m_score = compute_meteor_scores(references, hypotheses)\n",
        "\n",
        "    return np.mean(losses), np.mean(top5accs), bleu_scores, m_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOuO54pc-b2Q"
      },
      "outputs": [],
      "source": [
        "train_config = {\n",
        "    'embed_dim': 512,     # dim of word embedding\n",
        "    'attention_dim': 512, # dim of attention linear layers\n",
        "    'decoder_dim': 512,   # dim of decoder RNN\n",
        "    'encoder_dim': 1920,\n",
        "    'encoder_lr': 1e-4,   # lr for encoder if fine-tuning\n",
        "    'decoder_lr': 4e-4,   # lr for decoder\n",
        "    'grad_clip': 5.,      # clip gradients at an absolute value\n",
        "    'alpha_c': 1.,        # regularization parameter for 'doubly stochastic attention'\n",
        "    'num_epochs': 20,\n",
        "    'fine_tune_encoder': False,\n",
        "    'lr_decay_factor': 0.8,\n",
        "    'lr_decay_patience': 8,\n",
        "    'patience': 4\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXmdqRDny9CD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WQJuOQYC1rO"
      },
      "outputs": [],
      "source": [
        "# model parameters\n",
        "embed_dim = 512      # dim of word embeddings\n",
        "attention_dim = 512  # dim of attention linear layers\n",
        "decoder_dim = 512    # dim of decoder RNN\n",
        "encoder_dim = 1920   # 512 for VGG, 2048 for ResNet, 1920 for DenseNet, 1024 for GoogLeNet, 3 for AlexNet\n",
        "encoder_lr = 1e-4    # learning rate for encoder if fine-tuning\n",
        "decoder_lr = 4e-4    # learning rate for decoder\n",
        "grad_clip = 5.       # clip gradients at an absolute value of\n",
        "alpha_c = 1.         # regularization parameter for 'doubly stochastic attention'\n",
        "vocab_size = len(word2id)\n",
        "\n",
        "lr_decay_factor = 0.8\n",
        "lr_decay_patience = 8\n",
        "\n",
        "num_epochs = 20\n",
        "epochs_since_improvement = 0\n",
        "\n",
        "fine_tune_encoder = True  # fine-tune encoder?\n",
        "cudnn.benchmark = True     # set to true only if inputs to model are fixed size\n",
        "\n",
        "patience = 4\n",
        "\n",
        "# model setup\n",
        "decoder = Decoder(attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim)\n",
        "decoder_optimizer = optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()), lr=decoder_lr)\n",
        "\n",
        "encoder = ImageEncoder()\n",
        "encoder.fine_tune(fine_tune_encoder)\n",
        "encoder_optimizer = optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()), lr=encoder_lr) if fine_tune_encoder else None\n",
        "\n",
        "# Load checkpoint if existing\n",
        "checkpoint_path = '/content/drive/MyDrive/final_project/checkpoints/image_captioning_best.pth'\n",
        "\n",
        "if os.path.isfile(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
        "    print(f\"Loading checkpoint from {checkpoint_path} at epoch {checkpoint['epoch']}\")\n",
        "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "    decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
        "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "    if fine_tune_encoder:\n",
        "        encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
        "else:\n",
        "    print(f\"No checkpoint found at {checkpoint_path}. Starting from scratch.\")\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "# lr scheduler\n",
        "encoder_lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, mode='max', factor=lr_decay_factor, patience=lr_decay_patience) if fine_tune_encoder else None\n",
        "decoder_lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, mode='max', factor=lr_decay_factor, patience=lr_decay_patience)\n",
        "\n",
        "# criterion for loss\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# loop\n",
        "best_bleus = np.zeros(4)\n",
        "best_avg = 0.\n",
        "best_meteor = 0.\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    loss_train, acc_train = train_epoch(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer)\n",
        "    loss_val, acc_val, bleu_vals, m_score = val_epoch(val_loader, encoder, decoder, criterion)\n",
        "\n",
        "    # reduce the learning rate on plateau of BLEU-4 score only\n",
        "    decoder_lr_scheduler.step(bleu_vals[3])\n",
        "    if fine_tune_encoder:\n",
        "        encoder_lr_scheduler.step(bleu_vals[3])\n",
        "\n",
        "    # check if there was an improvement\n",
        "    score_avg = (np.sum(bleu_vals) + m_score) / 5\n",
        "    is_best = score_avg > best_avg\n",
        "    best_bleus = np.maximum(bleu_vals, best_bleus)\n",
        "    best_meteor = max(m_score, best_meteor)\n",
        "    best_avg = max(score_avg, best_avg)\n",
        "    if not is_best:\n",
        "        epochs_since_improvement += 1\n",
        "    else:\n",
        "        epochs_since_improvement = 0\n",
        "\n",
        "    print('-' * 40)\n",
        "    print(f'epoch: {epoch}, train loss: {loss_train:.4f}, train acc: {acc_train:.2f}%, valid loss: {loss_val:.4f}, valid acc: {acc_val:.2f}%, best BLEU-1: {best_bleus[0]:.4f}, best BLEU-2: {best_bleus[1]:.4f}, best BLEU-3: {best_bleus[2]:.4f}, best BLEU-4: {best_bleus[3]:.4f}, best METEOR: {best_meteor:.4f}')\n",
        "    print('-' * 40)\n",
        "    # save the checkpoint\n",
        "    save_checkpoint(\n",
        "        epoch=epoch,\n",
        "        epochs_since_improvement=epochs_since_improvement,\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        encoder_optimizer=encoder_optimizer if fine_tune_encoder else None,\n",
        "        decoder_optimizer=decoder_optimizer,\n",
        "        bleu_scores=bleu_vals,\n",
        "        meteor_score=m_score,\n",
        "        val_acc = acc_val,\n",
        "        train_acc = acc_train,\n",
        "        is_best=is_best,\n",
        "        ckpt_dir=\"/content/drive/MyDrive/final_project/checkpoints/\"\n",
        "    )\n",
        "\n",
        "    if epochs_since_improvement >= patience:\n",
        "        print(f\"Early stopping: No BLEU-4 improvement for {patience} epochs.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWW-8e5SOG4F"
      },
      "outputs": [],
      "source": [
        "epochs = []\n",
        "bleu1s, bleu2s, bleu3s, bleu4s = [], [], [], []\n",
        "meteor_scores = []\n",
        "val_scores = []\n",
        "train_scores = []\n",
        "\n",
        "ckpt_dir = \"/content/drive/MyDrive/final_project/checkpoints\"\n",
        "checkpoint_files = sorted(os.listdir(ckpt_dir))\n",
        "\n",
        "for ckpt_file in checkpoint_files[1:]:\n",
        "    print(ckpt_file)\n",
        "    if ckpt_file.endswith(\".pth\") or ckpt_file.endswith(\".pt\"):\n",
        "        checkpoint = torch.load(os.path.join(ckpt_dir, ckpt_file), weights_only=False)\n",
        "    else:\n",
        "        continue\n",
        "    bleu_scores = checkpoint['bleu-score']\n",
        "    meteor_score = checkpoint['meteor-score']\n",
        "    val_accuracy = checkpoint['validation-accuracy']\n",
        "    train_accuracy = checkpoint['training-accuracy']\n",
        "\n",
        "    epochs.append(checkpoint.get('epoch', len(epochs)))  # fallback if epoch isn't saved\n",
        "    bleu1s.append(bleu_scores[0])\n",
        "    bleu2s.append(bleu_scores[1])\n",
        "    bleu3s.append(bleu_scores[2])\n",
        "    bleu4s.append(bleu_scores[3])\n",
        "    meteor_scores.append(meteor_score)\n",
        "    val_scores.append(val_accuracy)\n",
        "    train_scores.append(train_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7idnsLaahvJ6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(epochs, bleu1s, label=\"BLEU-1\")\n",
        "plt.plot(epochs, bleu2s, label=\"BLEU-2\")\n",
        "plt.plot(epochs, bleu3s, label=\"BLEU-3\")\n",
        "plt.plot(epochs, bleu4s, label=\"BLEU-4\")\n",
        "plt.plot(epochs, meteor_scores, label=\"METEOR\", linestyle=\"--\")\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Model Performance Over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60kblvqh-Zng"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "print(val_scores)\n",
        "print(train_scores)\n",
        "plt.plot(epochs, val_scores, label=\"Validation Accuracy\")\n",
        "print(epochs)\n",
        "plt.plot(epochs, train_scores, label=\"Training Accuracy\")\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Performance Over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVV-s4nEzeLB"
      },
      "source": [
        "## Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNHoy13Ozd-5"
      },
      "outputs": [],
      "source": [
        "def _load_and_preprocess(image_path: str, image_size: int = 256):\n",
        "    \"\"\"Load image from disk, resize, normalize and return a tensor.\"\"\"\n",
        "    img = np.array(Image.open(image_path).convert('RGB'))\n",
        "    img = cv2.resize(img, (image_size, image_size))\n",
        "    assert img.shape == (image_size, image_size, 3)\n",
        "    assert np.max(img) <= 255\n",
        "\n",
        "    tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    return tf(img)  # (3, H, W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0HBfYc1-5uO"
      },
      "outputs": [],
      "source": [
        "def _plot_attention(image_path: str, caption: list, alphas: np.ndarray, grid_size: int, upsample: int = 24):\n",
        "    \"\"\"Display attention maps over the image for each word in caption.\"\"\"\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = img.resize([grid_size * upsample]*2, Image.LANCZOS)\n",
        "\n",
        "    num_words = len(caption)\n",
        "    cols = 5\n",
        "    rows = int(np.ceil(num_words / cols))\n",
        "    plt.figure(figsize=(cols*4, rows*3))\n",
        "\n",
        "    for t, word in enumerate(caption):\n",
        "        plt.subplot(rows, cols, t+1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(word, fontsize=12)\n",
        "\n",
        "        alpha_map = alphas[t + 1]\n",
        "        alpha_map = alpha_map.reshape(grid_size, grid_size)\n",
        "        alpha_up = skimage.transform.pyramid_expand(\n",
        "            alpha_map, upscale=upsample, sigma=8\n",
        "        )\n",
        "        plt.imshow(alpha_up, alpha=0.6, cmap='gray')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUmLYZOs_Fdy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def generate_caption(\n",
        "    encoder,\n",
        "    decoder,\n",
        "    image_path: str,\n",
        "    word_map: dict,\n",
        "    beam_size: int = 5,\n",
        "    max_caption_len: int = 50,\n",
        "    return_attention: bool = False,\n",
        "    device: str = \"cpu\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a caption for an image using beam search.\n",
        "    \"\"\"\n",
        "\n",
        "    if '<end>' in word_map:\n",
        "        end_idx = word_map['<end>']\n",
        "    elif '<eos>' in word_map:\n",
        "        end_idx = word_map['<eos>']\n",
        "    elif 'endseq' in word_map:\n",
        "        end_idx = word_map['endseq']\n",
        "    else:\n",
        "        raise KeyError(\n",
        "            \"No end-of-sentence token found in word_map. \"\n",
        "            \"Make sure one of '<end>', '<eos>', or 'endseq' is present.\"\n",
        "        )\n",
        "\n",
        "    # Load & preprocess image\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std =[0.229, 0.224, 0.225]\n",
        "    )\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256,256)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "    image = transform(img).unsqueeze(0).to(device) # (1,3,256,256)\n",
        "\n",
        "    # Encode\n",
        "    encoder_out = encoder(image) # (1, H, W, C)\n",
        "    encoder_out = encoder_out.permute(0, 3, 1, 2) # (1, C, H, W)\n",
        "    enc_dim     = encoder_out.size(1)\n",
        "    encoder_out = encoder_out.reshape(1, enc_dim, -1).permute(0,2,1) # (1, num_pixels, enc_dim)\n",
        "    num_pixels  = encoder_out.size(1)\n",
        "\n",
        "    # Beam search\n",
        "    encoder_out = encoder_out.expand(beam_size, num_pixels, enc_dim)\n",
        "    k = beam_size\n",
        "    vocab_size = len(word_map)\n",
        "    seqs = [[word_map['<start>']]] * k\n",
        "    top_k_scores = torch.zeros(k, 1).to(device)\n",
        "    seqs_alpha = torch.ones(k, 1, num_pixels).to(device)\n",
        "\n",
        "    complete_seqs = []\n",
        "    complete_seqs_alpha  = []\n",
        "    complete_seqs_scores = []\n",
        "\n",
        "    # hidden & cell from decoder’s init method\n",
        "    h, c = decoder.init_hidden_states(encoder_out)\n",
        "\n",
        "    step = 1\n",
        "    while True:\n",
        "        # last words\n",
        "        prev_words = torch.LongTensor([seq[-1] for seq in seqs]).to(device)\n",
        "        embeddings = decoder.embedding(prev_words)\n",
        "\n",
        "        # attention\n",
        "        context, alpha = decoder.attention(encoder_out, h)\n",
        "        gate = torch.sigmoid(decoder.beta_gate(h))\n",
        "        context = gate * context\n",
        "\n",
        "        # decode one step\n",
        "        h, c = decoder.decoder_cell(\n",
        "            torch.cat([embeddings, context], dim=1), (h, c)\n",
        "        )\n",
        "        scores = decoder.output_proj(h)\n",
        "        scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "        scores = top_k_scores.expand_as(scores) + scores\n",
        "\n",
        "        if step == 1:\n",
        "            scores = scores[0]\n",
        "\n",
        "        # flatten & top-k\n",
        "        scores, indices = scores.view(-1).topk(\n",
        "            k, dim=0, largest=True, sorted=True\n",
        "        )\n",
        "        prev_beams = indices // vocab_size\n",
        "        next_words = indices %  vocab_size\n",
        "\n",
        "        # build new beams\n",
        "        new_seqs = []\n",
        "        new_seqs_alpha = []\n",
        "        for b, w in zip(prev_beams, next_words):\n",
        "            new_seqs.append(seqs[b] + [w.item()])\n",
        "            new_seqs_alpha.append(\n",
        "                torch.cat([seqs_alpha[b], alpha[b].unsqueeze(0)], dim=0)\n",
        "            )\n",
        "\n",
        "        seqs = []\n",
        "        seqs_alpha = []\n",
        "        for i, seq in enumerate(new_seqs):\n",
        "            if seq[-1] == end_idx:\n",
        "                complete_seqs.append(seq)\n",
        "                complete_seqs_alpha.append(new_seqs_alpha[i])\n",
        "                complete_seqs_scores.append(scores[i])\n",
        "            else:\n",
        "                seqs.append(seq)\n",
        "                seqs_alpha.append(new_seqs_alpha[i])\n",
        "\n",
        "        k = len(seqs)\n",
        "        if k == 0 or step >= max_caption_len:\n",
        "            break\n",
        "\n",
        "        h = h[prev_beams[:k]]\n",
        "        c = c[prev_beams[:k]]\n",
        "        encoder_out = encoder_out[prev_beams[:k]]\n",
        "        top_k_scores = scores[:k].unsqueeze(1)\n",
        "        step += 1\n",
        "\n",
        "    if not complete_seqs_scores:\n",
        "        complete_seqs = seqs\n",
        "        complete_seqs_alpha = seqs_alpha\n",
        "        complete_seqs_scores = [s.item() for s in top_k_scores.squeeze(1)]\n",
        "\n",
        "    best_idx = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "    best_seq = complete_seqs[best_idx]\n",
        "    alphas = complete_seqs_alpha[best_idx].cpu().detach().numpy()\n",
        "\n",
        "    idx2word = {v: k for k, v in word_map.items()}\n",
        "    caption  = []\n",
        "    for idx in best_seq:\n",
        "        w = idx2word[idx]\n",
        "        if w == '<start>': continue\n",
        "        if idx == end_idx:   break\n",
        "        caption.append(w)\n",
        "\n",
        "    return (caption, alphas) if return_attention else caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04V-aLPtVxMx"
      },
      "outputs": [],
      "source": [
        "def display_caption_image(image_path, beam_size=5):\n",
        "  # Display Image\n",
        "  img = Image.open(image_path).convert(\"RGB\")\n",
        "  plt.figure(figsize=(6,6))\n",
        "  plt.imshow(img)\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(\"Input Image\")\n",
        "  plt.show()\n",
        "  # Run inference and plot attention\n",
        "  caption, alphas = generate_caption(\n",
        "      encoder=encoder,\n",
        "      decoder=decoder,\n",
        "      image_path=image_path,\n",
        "      word_map=word2id,\n",
        "      beam_size=beam_size,\n",
        "      return_attention=True,\n",
        "      device=device,\n",
        "  )\n",
        "  # Print out the final caption\n",
        "  print(\"Predicted caption:\")\n",
        "  print(\" \".join(caption))\n",
        "\n",
        "  _plot_attention(image_path, caption, alphas, grid_size=14)\n",
        "  # return caption, alphas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUx0tM8pWSk2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyper-params (must match training)\n",
        "embed_dim     = 512\n",
        "attention_dim = 512\n",
        "decoder_dim   = 512\n",
        "encoder_dim   = 1920\n",
        "vocab_size    = len(word2id)\n",
        "\n",
        "# Instantiate models\n",
        "encoder = ImageEncoder(output_size=14).to(device)\n",
        "decoder = Decoder(\n",
        "    attention_dim=attention_dim,\n",
        "    embed_dim=embed_dim,\n",
        "    decoder_dim=decoder_dim,\n",
        "    vocab_size=vocab_size,\n",
        "    encoder_dim=encoder_dim,\n",
        ").to(device)\n",
        "\n",
        "# Load checkpoint\n",
        "ckpt_path = Path(\"/content/drive/MyDrive/final_project/checkpoints/image_captioning_best.pth\")\n",
        "ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
        "\n",
        "# Load state\n",
        "encoder.load_state_dict(ckpt[\"encoder_state_dict\"])\n",
        "decoder.load_state_dict(ckpt[\"decoder_state_dict\"])\n",
        "bleu_score = ckpt[\"bleu-score\"]\n",
        "print(f\"Loaded checkpoint from epoch {ckpt['epoch']}: \\\n",
        "\\n BLEU1-score={bleu_score[0]:.4f}, BLEU2-score={bleu_score[1]:.4f}, BLEU3-score={bleu_score[2]:.4f}, BLEU4-score={bleu_score[3]:.4f}, METEOR-score={ckpt['meteor-score']:.4f}\")\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4qenxjeVUOZ"
      },
      "outputs": [],
      "source": [
        "# Choose image and beam size\n",
        "image_path = \"/content/Flicker8k_Dataset/1000268201_693b08cb0e.jpg\"\n",
        "beam_size  = 5\n",
        "display_caption_image(image_path, beam_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI-zS5AgPJs_"
      },
      "outputs": [],
      "source": [
        "image_path = \"/content/Flicker8k_Dataset/1470132731_fa416b7504.jpg\"\n",
        "beam_size  = 5\n",
        "display_caption_image(image_path, beam_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcefXT9zaxQq"
      },
      "outputs": [],
      "source": [
        "image_path = \"/content/Flicker8k_Dataset/1468962616_5803b4397f.jpg\"\n",
        "beam_size  = 5\n",
        "display_caption_image(image_path, beam_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLIzyu0OcYJu"
      },
      "outputs": [],
      "source": [
        "image_path = \"/content/Flicker8k_Dataset/1819261140_6c022f4b1d.jpg\"\n",
        "beam_size  = 5\n",
        "display_caption_image(image_path, beam_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_AoeYyccnqK"
      },
      "outputs": [],
      "source": [
        "image_path = \"/content/Flicker8k_Dataset/2021602343_03023e1fd1.jpg\"\n",
        "beam_size  = 5\n",
        "display_caption_image(image_path, beam_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}