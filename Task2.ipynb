{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression â€“ Gold price prediction**"
      ],
      "metadata": {
        "id": "YmviP2IADYFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries and Start PySpark Session"
      ],
      "metadata": {
        "id": "p9f7GkUakP7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lag, window, to_date, monotonically_increasing_id\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "spark = SparkSession.builder.appName(\"GoldPricePrediction\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "QBWZ9VjakXZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preprocessing"
      ],
      "metadata": {
        "id": "FV5SYRkOj3gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data and Preprocessing\n",
        "try:\n",
        "    df = spark.read.csv(\n",
        "        \"gold_prices.csv\",\n",
        "        header=True,\n",
        "        inferSchema=True\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Error reading the CSV file: {e}\")\n",
        "    spark.stop()\n",
        "    exit()\n",
        "\n",
        "# Rename columns to remove spaces\n",
        "df = df.withColumnRenamed(\"Buy Price\", \"BuyPrice\") \\\n",
        "       .withColumnRenamed(\"Sell Price\", \"SellPrice\")\n",
        "\n",
        "# Convert 'Date' column to date type\n",
        "df = df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy/MM/dd\"))\n",
        "\n",
        "# Filter out rows with null dates if any, and sort by date\n",
        "df = df.filter(col(\"Date\").isNotNull()).orderBy(\"Date\")\n",
        "\n",
        "# For this task, we will use 'SellPrice' as the target gold price.\n",
        "# Let's also ensure the prices are numeric (double or float).\n",
        "df = df.withColumn(\"SellPrice\", col(\"SellPrice\").cast(\"double\"))\n",
        "df = df.withColumn(\"BuyPrice\", col(\"BuyPrice\").cast(\"double\"))\n",
        "\n",
        "# Drop rows with null prices if they exist after casting\n",
        "df = df.dropna(subset=[\"SellPrice\"])\n",
        "\n",
        "# Add an index column to preserve order for window functions\n",
        "df = df.withColumn(\"index\", monotonically_increasing_id())\n"
      ],
      "metadata": {
        "id": "ZaxdUJCyj_eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generate Features and Label"
      ],
      "metadata": {
        "id": "6FcNgJBhkgp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Generate Features and Labels\n",
        "# Define a window specification ordered by 'Date' and then 'index'\n",
        "window_spec = Window.orderBy(\"Date\", \"index\")\n",
        "\n",
        "# Create lagged columns for the previous 10 days' SellPrice\n",
        "# The label will be the current day's 'SellPrice'\n",
        "df_features = df.withColumn(\"label\", col(\"SellPrice\"))\n",
        "\n",
        "# Generate features: gold prices of 10 consecutive previous dates\n",
        "feature_cols = []\n",
        "for i in range(1, 11):\n",
        "    feature_name = f\"price_lag_{i}\"\n",
        "    # lag(\"SellPrice\", i) means price from i days ago\n",
        "    df_features = df_features.withColumn(feature_name, lag(\"SellPrice\", i).over(window_spec))\n",
        "    feature_cols.append(feature_name)\n",
        "\n",
        "# Remove rows with null values in features (these will be the first 10 rows due to lag)\n",
        "df_features = df_features.dropna(subset=feature_cols)\n",
        "\n",
        "if df_features.count() == 0:\n",
        "    print(\"No data available after feature engineering. This might be due to insufficient data points for the 10-day lag or issues with data.\")\n",
        "    df.show(15) # Show some initial data for debugging\n",
        "    spark.stop()\n",
        "    exit()\n"
      ],
      "metadata": {
        "id": "Jw5iG2Z5klkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prepare Data for Spark ML"
      ],
      "metadata": {
        "id": "OVyYuEsDlAor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\") # Added handleInvalid\n",
        "df_assembled = assembler.transform(df_features)\n",
        "\n",
        "# Select only 'features' and 'label' columns\n",
        "final_df = df_assembled.select(\"features\", \"label\")"
      ],
      "metadata": {
        "id": "IuxeMtAMlEel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Split Data into Training and Test Sets\n"
      ],
      "metadata": {
        "id": "clYW5N-ZlT_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = final_df.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Cache the dataframes for performance\n",
        "train_df.cache()\n",
        "test_df.cache()\n",
        "\n",
        "if train_df.count() == 0 or test_df.count() == 0:\n",
        "    print(\"Training or testing dataframe is empty after split. Check data and feature engineering steps.\")\n",
        "    print(f\"Total samples before split: {final_df.count()}\")\n",
        "    print(f\"Training samples: {train_df.count()}, Test samples: {test_df.count()}\")\n",
        "    spark.stop()\n",
        "    exit()\n"
      ],
      "metadata": {
        "id": "7C2Az4YilWQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Train Linear Regression Model\n"
      ],
      "metadata": {
        "id": "6mqReacTlmwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\", solver=\"l-bfgs\", maxIter=100) # maxIter can be adjusted\n",
        "# Train the model\n",
        "try:\n",
        "    lr_model = lr.fit(train_df)\n",
        "except Exception as e:\n",
        "    print(f\"Error during model training: {e}\")\n",
        "    if \"requirement failed: Column features must be of type equal to one of the following types\" in str(e):\n",
        "        print(\"This error often means the 'features' column was not created correctly or has an unexpected data type.\")\n",
        "        print(\"Schema of train_df:\")\n",
        "        train_df.printSchema()\n",
        "        print(\"Sample of train_df (first 5 rows):\")\n",
        "        train_df.show(5, truncate=False)\n",
        "    elif \"Nothing has been added to this summarizer.\" in str(e):\n",
        "        print(\"This error often means the training dataframe is empty or has no valid features/labels.\")\n",
        "        print(f\"Training data count: {train_df.count()}\")\n",
        "        train_df.show(5,truncate=False)\n",
        "    spark.stop()\n",
        "    exit()"
      ],
      "metadata": {
        "id": "N7-Jr2DTlpdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluate Model"
      ],
      "metadata": {
        "id": "94qemdRTl6U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Make predictions on training and test sets\n",
        "train_predictions = lr_model.transform(train_df)\n",
        "test_predictions = lr_model.transform(test_df)\n",
        "\n",
        "# Evaluator\n",
        "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "evaluator_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "\n",
        "# Evaluate on training data\n",
        "train_rmse = evaluator_rmse.evaluate(train_predictions)\n",
        "train_r2 = evaluator_r2.evaluate(train_predictions)\n",
        "\n",
        "# Evaluate on test data\n",
        "test_rmse = evaluator_rmse.evaluate(test_predictions)\n",
        "test_r2 = evaluator_r2.evaluate(test_predictions)\n",
        "\n",
        "print(f\"\\n--- Model Evaluation ---\")\n",
        "print(f\"Training Set RMSE: {train_rmse:.4f}, R2: {train_r2:.4f}\")\n",
        "print(f\"Test Set RMSE: {test_rmse:.4f}, R2: {test_r2:.4f}\")\n",
        "\n",
        "# 7. Visualize Results\n",
        "\n",
        "# a. Line chart for losses during the training process\n",
        "training_summary = lr_model.summary\n",
        "objective_history = training_summary.objectiveHistory\n",
        "\n",
        "# Check if objective history is available and not empty\n",
        "if objective_history:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(objective_history)\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Objective Function (RMSE)\") # For LinearRegression, objectiveHistory is RMSE on training data\n",
        "    plt.title(\"Training Objective History (RMSE)\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"training_loss_chart.png\")\n",
        "    # plt.show() # In a script, plt.show() might block execution. Saving is usually preferred.\n",
        "    print(\"\\nTraining loss chart (training_loss_chart.png) generated.\")\n",
        "else:\n",
        "    print(\"\\nObjective history not available or empty. Cannot plot training loss.\")\n",
        "\n",
        "\n",
        "# b. Bar chart to contrast results (RMSE) in training and test sets\n",
        "metrics_data_pd = {\n",
        "    'Set': ['Training', 'Test'],\n",
        "    'RMSE': [train_rmse, test_rmse]\n",
        "}\n",
        "metrics_df_pd_df = pd.DataFrame(metrics_data_pd) # Renamed to avoid conflict\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "bars = plt.bar(metrics_df_pd_df['Set'], metrics_df_pd_df['RMSE'], color=['skyblue', 'lightcoral'])\n",
        "plt.xlabel(\"Data Set\")\n",
        "plt.ylabel(\"Root Mean Squared Error (RMSE)\")\n",
        "plt.title(\"RMSE Comparison: Training vs. Test Set\")\n",
        "plt.ylim(0, max(train_rmse, test_rmse) * 1.2 if max(train_rmse, test_rmse) > 0 else 1) # Adjust y-limit\n",
        "\n",
        "# Add RMSE values on top of the bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01 * max(train_rmse, test_rmse, 1) , f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.savefig(\"rmse_comparison_chart.png\")\n",
        "# plt.show()\n",
        "print(\"RMSE comparison bar chart (rmse_comparison_chart.png) generated.\")\n",
        "\n",
        "# convert to Pandas for easier plotting (limit for performance)\n",
        "sample_test_pd = test_predictions.select(\"label\", \"prediction\").limit(100).toPandas()\n",
        "\n",
        "if not sample_test_pd.empty:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(sample_test_pd.index, sample_test_pd['label'], label='Actual Prices', marker='o', linestyle='-')\n",
        "    plt.plot(sample_test_pd.index, sample_test_pd['prediction'], label='Predicted Prices', marker='x', linestyle='--')\n",
        "    plt.xlabel(\"Sample Index (from Test Set)\")\n",
        "    plt.ylabel(\"Gold Price\")\n",
        "    plt.title(\"Actual vs. Predicted Gold Prices (Sample from Test Set)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"actual_vs_predicted_test_sample.png\")\n",
        "    # plt.show()\n",
        "    print(\"Actual vs. Predicted prices chart (actual_vs_predicted_test_sample.png) generated for a sample of the test set.\")\n",
        "else:\n",
        "    print(\"Test predictions sample is empty, skipping actual vs. predicted plot.\")\n",
        "\n",
        "# Stop Spark Session\n",
        "spark.stop()\n",
        "\n",
        "print(\"\\nProcess completed successfully.\")\n",
        "print(f\"Please check the generated image files: training_loss_chart.png, rmse_comparison_chart.png, and actual_vs_predicted_test_sample.png (if data was sufficient).\")"
      ],
      "metadata": {
        "id": "v1hFwqdol9Ma"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}